{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Nadi Furkan Açıkgöz, 20170808059\n",
    "# Ahmet Gültekin, 20170808009"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CME193 - Assignment 1\n",
    "\n",
    "In this assignment you will be learning about and implementing [Markov Chains](https://en.wikipedia.org/wiki/Markov_chain), if you have never heard  of them before, do not worry, this notebook will explain everything needed for the assignment.\n",
    "\n",
    "## Markov chains\n",
    "The best way to introduce what a Markov chain is with a simple example. Suppose you have a simple counter that you can set to any number between 0 and 9 (including 0 and 9). At every step you roll a standard 6 -sided die and depending on whether the rolled number is even or odd you either increment or decrement the counter. If you try to increment at 9, lets say it wraps around to 0 and similarly for trying to decrement at 0. Now you can play this game for multiple steps and record the counter value after every step, this is just a simple markov chain.\n",
    "\n",
    "Here is an example evolution of the markov chain,\n",
    "Start with counter at 1.\n",
    "- Step 1 : Roll a 4, its even so increment the counter to 2.\n",
    "- Step 2 : Roll a 2, its even so increment the counter to 3.\n",
    "- Step 3 : Roll a 5, its odd so decrement the counter to 2.\n",
    "\n",
    "etc.\n",
    "\n",
    "A markov chain consists of a set of n states (in our example it was the 10 states of the counter) and a probabilistic rule to jump to another state every step (the dice rolls). Although in our simple example we had a similar rule for each state, you can have a different rule for each state. For example you may say that if you the counter is odd, dont roll a die, always increment, and follow the die for even states. The rule is always in the form of n probabilities that sum to one. Each probability indicates the chance that it jumps to that state.\n",
    "\n",
    "In our example the rule for state 0 can be represented by the below numpy vector\n",
    "\n",
    "When you are in state 0, you have 50% chance of landing on state 1 (you rolled even) or a 50% chance of landing in state 9 ( you rolled odd and tried to decrement at 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0. , 0.5, 0. , 0. , 0. , 0. , 0. , 0. , 0. , 0.5])"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "np.array([0. , 0.5, 0. , 0. , 0. , 0. , 0. , 0. , 0. , 0.5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The index in the array represents the state and the value represents the probability with which you will jump to that state. Similarly this is the rule for state 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0. , 0. , 0. , 0.5, 0. , 0.5, 0. , 0. , 0. , 0. ])"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.array([0. , 0. , 0. , 0.5, 0. , 0.5, 0. , 0. , 0. , 0. ])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transition Matrix\n",
    "\n",
    "The way to represent markov chains is with a transition matrix $T$ of size (n,n), where the column i will the rule vector for the state i ( Here we are assuming that the states ae numbered 0 to n-1 ).\n",
    "\n",
    "Thus you can interpret the element $T_{ij}$ of the matrix as the probability you will jump to state i from the state j."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 1\n",
    "\n",
    "Write a function to return the transition matrix for the following markov chain, given the number of states as an argument.\n",
    "\n",
    "The rule for the markov chain is as follows,\n",
    "\n",
    "If current state is i (and if i is not n-1)\n",
    "1. With probability 0.6, it goes to state i+1\n",
    "2. With probability 0.1, it will fall back to state i = 0.\n",
    "2. With probability 0.3, it goes to state i-1 if i is at least 1. If i is instead 0 it stays at state 0.\n",
    "3. At state n-1, instead of the above operations we stay at state n-1 with probability 1\n",
    "\n",
    "Intuitively, this markov chain behaves in the following way, at every step there is an 60% chance that it climbs up by 1, a 10% chance that it falls down completely, and a 30% chance it falls down by 1. If it does reach the final state n-1, it stays there forever.\n",
    "\n",
    "You are **not** allowed to use any for loops for this question, instead use numpy indexing to fill in your matrix. You can assume that n is atleast 4.\n",
    "\n",
    "Hints:\n",
    "1. First write your code in an empty cell for with a hardcoded n value, in this way you can quickly iterate and get the right answer first. Then copy it into the function.\n",
    "2. You can use [Array indexing](https://docs.scipy.org/doc/numpy-1.13.0/user/basics.indexing.html#index-arrays)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "##### YOUR CODE HERE\n",
    "def transition_matrix(n):\n",
    "    matrix = np.zeros((n,n))\n",
    "    # np.eye() method is used to create a matrix in given size which the diagonal values are the specified value.\n",
    "    matrix[1:, :n-1] = np.eye(n-1) * 0.6\n",
    "    matrix[1:n-2, 2:n-1] += np.eye(n-3) * 0.3\n",
    "    # np.full() method is used to create an n dimensional array whose all the values are specified number.\n",
    "    matrix[0, :n-1] = np.full(n-1, 0.1)\n",
    "    matrix[0,:2] = np.full(2, 0.4)\n",
    "    matrix[n-1, n-1] = 1\n",
    "    return matrix\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.4, 0.4, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0. ],\n",
       "       [0.6, 0. , 0.3, 0. , 0. , 0. , 0. , 0. , 0. , 0. ],\n",
       "       [0. , 0.6, 0. , 0.3, 0. , 0. , 0. , 0. , 0. , 0. ],\n",
       "       [0. , 0. , 0.6, 0. , 0.3, 0. , 0. , 0. , 0. , 0. ],\n",
       "       [0. , 0. , 0. , 0.6, 0. , 0.3, 0. , 0. , 0. , 0. ],\n",
       "       [0. , 0. , 0. , 0. , 0.6, 0. , 0.3, 0. , 0. , 0. ],\n",
       "       [0. , 0. , 0. , 0. , 0. , 0.6, 0. , 0.3, 0. , 0. ],\n",
       "       [0. , 0. , 0. , 0. , 0. , 0. , 0.6, 0. , 0.3, 0. ],\n",
       "       [0. , 0. , 0. , 0. , 0. , 0. , 0. , 0.6, 0. , 0. ],\n",
       "       [0. , 0. , 0. , 0. , 0. , 0. , 0. , 0. , 0.6, 1. ]])"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#TESTING\n",
    "transition_matrix(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To test your function , check that the above returns \n",
    "```\n",
    "array([[0.4, 0.4, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0. ],\n",
    "       [0.6, 0. , 0.3, 0. , 0. , 0. , 0. , 0. , 0. , 0. ],\n",
    "       [0. , 0.6, 0. , 0.3, 0. , 0. , 0. , 0. , 0. , 0. ],\n",
    "       [0. , 0. , 0.6, 0. , 0.3, 0. , 0. , 0. , 0. , 0. ],\n",
    "       [0. , 0. , 0. , 0.6, 0. , 0.3, 0. , 0. , 0. , 0. ],\n",
    "       [0. , 0. , 0. , 0. , 0.6, 0. , 0.3, 0. , 0. , 0. ],\n",
    "       [0. , 0. , 0. , 0. , 0. , 0.6, 0. , 0.3, 0. , 0. ],\n",
    "       [0. , 0. , 0. , 0. , 0. , 0. , 0.6, 0. , 0.3, 0. ],\n",
    "       [0. , 0. , 0. , 0. , 0. , 0. , 0. , 0.6, 0. , 0. ],\n",
    "       [0. , 0. , 0. , 0. , 0. , 0. , 0. , 0. , 0.6, 1. ]])\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 2\n",
    "\n",
    "Now we want to use the transtion matrix to know what are the probabilites of being in each state after running the markov chain for k steps. For example, for the above markov chain, I want to know after 30 steps, what is the probablity that I have reached the state n-1.\n",
    "\n",
    "Luckily this is fairly easy to calculate, using matrix multiplication. An amazingly useful property of the transition matrix is as follows, if you have vector $p_j$ as the vector of probabilites after step $j$, then $p_{j+1}$ is given by the following matrix vector product.\n",
    "\n",
    "$$ p_{j+1} =  Tp_j $$\n",
    "\n",
    "We will not prove this result here, but if you spend some time thinking about what matrix multiplication is doing here, especially interpret it as taking a linear combination of columns then it should make sense.\n",
    "\n",
    "Thus if someone gave you a probability distribution for the starting state as $p_0$, then the probability distribtion after $k$ steps is simply given by multiplying by the transition matrix k times\n",
    "\n",
    "$$ p_{k} =  T^{k}p_0 $$\n",
    "\n",
    "Implement a function which given an initial distribution `p0`, number of steps `k` and a transition matrix `tm`, it will return the distribution after k steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This propagate method simply calculates the probability distribution which tell us the probability\n",
    "# of being in each state after k steps.\n",
    "def propagate(p0,k,tm):\n",
    "    for i in range(k):\n",
    "        # np.dot() method is used for matrix to vector multiplication.\n",
    "        p0 = np.dot(tm, p0)\n",
    "    return p0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.16029774, 0.13006664, 0.10561269, 0.08547506, 0.06947427,\n",
       "       0.05552321, 0.04450658, 0.03281234, 0.02049648, 0.29573499])"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#TESTING\n",
    "tm = transition_matrix(10)\n",
    "p0 = np.zeros(10)\n",
    "p0[0] = 1\n",
    "pk = propagate(p0,30,tm)\n",
    "pk"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above code should return :\n",
    "\n",
    "```\n",
    "array([0.16029774, 0.13006664, 0.10561269, 0.08547506, 0.06947427,\n",
    "       0.05552321, 0.04450658, 0.03281234, 0.02049648, 0.29573499])\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 3\n",
    "### Part a\n",
    "With this implementation of `propagate`, you can now compute the probability of being in each state of the markov chain after k steps. How many steps does it take for the probability of being in the final state to be at least 0.5?\n",
    "\n",
    "After each step the probability of being in the final state slowly increases: once we are in state n-1, we never leave. At some point this probability will cross 50%. What you need to calculate is how many steps does it take to cross 50%.\n",
    "\n",
    "Again use n=10 transition matrix and start from state 0.\n",
    "\n",
    "Hints:\n",
    "1. Use the while loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "At 50th step probability of being in the last state is at least 0.5\n"
     ]
    }
   ],
   "source": [
    "# Creating Transition Matrix\n",
    "tm = transition_matrix(10)\n",
    "# Creating probability distribution of initial state\n",
    "p0 = np.zeros(10)\n",
    "p0[0] = 1\n",
    "k = 1\n",
    "\n",
    "\n",
    "while True:\n",
    "    # In each iteration propagate() method with the k value of 1, pass to next state probability distribution.\n",
    "    p0 = propagate(p0,1,tm)\n",
    "    # And here we check probability of being in the last state, if it is less than 0.5\n",
    "    # we go to next step and increase k by 1.\n",
    "    if p0[9] < 0.5:\n",
    "        k += 1\n",
    "    else:\n",
    "        break\n",
    "print(f\"At {k}th step probability of being in the last state is at least 0.5\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part b\n",
    "Convert the above Code into a function that computes the number of steps to 50% probability in the final state given `n` the size of the markov chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The difference in this method is we give the size of markov chain in other words the number of states into function\n",
    "# as a parameter. \n",
    "def num_steps(n):\n",
    "    tm = transition_matrix(n)\n",
    "    p0 = np.zeros(n)\n",
    "    p0[0] = 1\n",
    "    k = 1\n",
    "\n",
    "    while True:\n",
    "        p0 = propagate(p0,1,tm)\n",
    "        if p0[n-1] < 0.5:\n",
    "            k += 1\n",
    "        else:\n",
    "            break\n",
    "    return k"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part c\n",
    "Compute and plot the number of steps required for $n$ ranging from 10 to 40.\n",
    "\n",
    "Lookup the plotting function `plt.semilogy`and understand what it does, plot another graph of the number of steps using it.\n",
    "\n",
    "What does the semilogy plot tell you?\n",
    "\n",
    "Save the plots as \"qsn3c.png\" and \"qsn3c_semilogy.png\"\n",
    "\n",
    "**Hint:** if your code takes a long time to run, your implementation of `num_steps` may need to be improved. If you  know the distribution after $j$ steps of the markov chain, is there a way to use it to compute the distribution after $j+1$ steps?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "plt.semilogy method visualize the data with 2D lines. It has linear x axis and logarithmic y axis. Semilogy plot tells me that how the number of steps changing to be in the last state according to the size of markov chain."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "num_of_steps = [num_steps(n) for n in range(10,41)]\n",
    "rangeArray = np.arange(10,41)\n",
    "\n",
    "plt.clf()\n",
    "plt.plot(rangeArray,num_of_steps)\n",
    "plt.savefig(\"qsn3c.png\")\n",
    "plt.figure()\n",
    "plt.semilogy(rangeArray,num_of_steps)\n",
    "plt.savefig(\"qsn3c_semilogy.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 4\n",
    "\n",
    "### Part a\n",
    "Now we want to also compute some samples from the markov chain. What does this mean? Given a starting state, the Markov chain says that we move to various different states with some probabilities. We want to use randomness to choose what this next state is. Each call to the function will return a different answer, a new sample.\n",
    "\n",
    "For example consider the markov chain we have been using. It computes the next state by either adding 1 (with probability 0.6), subtracting 1 (probability 0.3), or going back to state 0 (probability 0.1) until it reaches the final state. Once it reaches the final state it stays there forever. If we were starting from state `4` of our Markov chain (for example), then we want to return `5` with probability 0.6, `3` with probability 0.3, and `0` with probability 0.1. \n",
    "\n",
    "Write a function `sample` which takes in a transition matrix `tm` and an initial state `s0` and samples a state to move to starting from `s0`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy.random as rnd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# This method randomly choose one element among the provided list based on the probabilites of each one. P parameter\n",
    "# provides what is the probability of each element.\n",
    "def sample(tm, s0):\n",
    "    return rnd.choice(np.arange(len(tm)), p = tm[:,s0])\n",
    "\n",
    "sample(tm,8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Your implementation should **not** hard-code the probability values from the specific Markov chain we have been working with so far."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### part b\n",
    "\n",
    "With this implementation of `sample`, write a function `sample_k_steps` which runs a random walk in the chain for `k` steps from an initial state `s0`. The random walk works in the following way: we start from `s0` and sample a new state to move to. We then set that new state to be `s0` and sample a new state to move to from there, repeating this until we have stepped `k` times.\n",
    "\n",
    "As an example, in the chain we have been working with so far we can only add 1, subtract 1, or go to 0 each time we step until we reach state `n-1`. When we are in state `n-1` we stay there forever. Thus a potential sample of 25 steps in the `n=10` chain is\n",
    "```\n",
    "array([0,1,2,1,2,3,0,0,1,2,3,4,5,4,3,4,5,6,7,8,9,9,9,9,9,9])\n",
    "```\n",
    "Note that each element of this array is equal to 0, equal to n-1, or is 1 greater or 1 less than the previous one. Further once we have an `n-1` value in our array we have that value for all future elements. \n",
    "\n",
    "Write a function `sample_k_steps` which takes in transition matrix `tm`, number of steps `k`, and  starting state `s0` and returns an array of `k+1` states sampled from the chain.\n",
    "\n",
    "Hint: You can use `sample` from 4a for this"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[4, 5, 6, 7, 8, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9]\n"
     ]
    }
   ],
   "source": [
    "# This method creates a sample consist of states. In other words it is the journey which the state takes to reach to \n",
    "# last state from the specified initial state. If the limit is lower than the number of steps to reach last state then\n",
    "# it stops when the exceed.\n",
    "def sample_k_steps(tm,k,s0):\n",
    "    states = [s0]\n",
    "\n",
    "    for i in range(k):\n",
    "        s_next = sample(tm,s0)\n",
    "        states.append(s_next)\n",
    "        s0 = s_next\n",
    "\n",
    "    return states\n",
    "\n",
    "print(sample_k_steps(tm,30,4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use the below code to test out some of the samples, it samples 20 trajectories of the markov chain and plots them. The plot should show that almost all or all the trajectories would ahve reached the final state by the end. Save this graph as \"qsn4b.png\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TESTING\n",
    "tm = transition_matrix(10)\n",
    "k = 200\n",
    "s0 = 0\n",
    "plt.clf()\n",
    "\n",
    "for i in range(20):\n",
    "    states = sample_k_steps(tm, k, s0)\n",
    "    plt.plot(states)\n",
    "\n",
    "plt.savefig(\"qsn4b.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 5\n",
    "\n",
    "We will now use our implementation of `sample` to estimate *hitting times* in a Markov chain. The hitting time between states `i` and `j` is the average number of steps that a random walk starting from state `i` takes to hit state `j`. We will be creating a [Monte Carlo simulation](https://en.wikipedia.org/wiki/Monte_Carlo_method) to estimate this value: if we run many random walks starting from `i` and count the number of steps it takes them to reach `j`, the average number of steps we observe is (hopefully) a good approximation to the true hitting time.\n",
    "\n",
    "Write a function `hitting_time_1_walk` which takes in as arguments a transition matrix `tm`, a starting point `i`, and an ending point `j`. Your function should run a random walk starting from `i` until it hits `j`, and return the number of steps that your walk takes to hit `j`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#YOUR CODE HERE\n",
    "def hitting_time_1_walk(tm,i,j):\n",
    "    # s is the current state\n",
    "    s = i\n",
    "    walk = [s]\n",
    "\n",
    "    while s != j:\n",
    "        # Here with sample() method current state moves the next state and append it to walk array.\n",
    "        s = sample(tm, s)\n",
    "        walk.append(s)\n",
    "    return len(walk)\n",
    "\n",
    "hitting_time_1_walk(tm,0,5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that when applied to our markov chain from before, `hitting_time_1_walk(tm,0,n-1)` runs a random walk starting from `0` until it hits the end state `n-1`. Consequently, we expect that `hitting_time_1_walk(tm,0,n-1)` will be at most some value `k` with probability equal to `propagate(tm,0,k)[n-1]` (take some time to convince yourself of this if you are not sure of this).\n",
    "\n",
    "1. **Create a new transition matrix `tm` for n = 20**, and create an array of 1000 calls to `hitting_time_1_walk(tm,0,n-1)`. \n",
    "\n",
    "2. Write a function `proportion_at_most` which takes in as arguments an array `arr` and a value `max_value`. It should return an array with `max_value` entries, where the `i`'th entry of the output array is the proportion of the values from `arr` which are at most `i`. For example, `proportion_at_most(arr=[1,2,4,2,6], max_value = 7)` should return `[0,0.2,0.6,0.6,0.8,0.8,1,1]` (or the numpy array equivalent to this)\n",
    "\n",
    "\n",
    "3. Apply this function to the hitting time samples you computed, with `max_value = 2000`.\n",
    "\n",
    "4. Compute the theoretical probability of being in state `n-1` after `k` steps using `propagate`, for every value of `k` between `0` and `2000`. \n",
    "\n",
    "5. Plot the estimated probabilities from 2) and the true probabilities from 3) against each other. Do they agree?\n",
    "\n",
    "\n",
    "Save your plot as `qsn5.png`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "# true value\n",
    "tm = transition_matrix(20)\n",
    "s0 = np.zeros(20)\n",
    "s0[0] = 1\n",
    "true_arr = []\n",
    "\n",
    "for i in range(1000):\n",
    "    true_arr.append(hitting_time_1_walk(tm,0,19))\n",
    "\n",
    "def true_proportion_at_most(true_arr,max_value):\n",
    "    result = []\n",
    "\n",
    "    for i in range(max_value+1):\n",
    "        num = 0\n",
    "        for j in range(len(true_arr)):\n",
    "            if true_arr[j] <= i:\n",
    "                num += 1\n",
    "        result.append(num/len(true_arr))    \n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "# estimated \n",
    "tm = transition_matrix(20)\n",
    "s0 = np.zeros(20)\n",
    "s0[0] = 1\n",
    "estimated_arr = []\n",
    "\n",
    "for i in range(1000):\n",
    "    k = hitting_time_1_walk(tm,0,19)\n",
    "    estimated_arr.append(propagate(s0,k,tm)[19])\n",
    "\n",
    "def estimated_proportion_at_most(estimated_arr,max_value):\n",
    "    result = []\n",
    "\n",
    "    for i in range(max_value+1):\n",
    "        num = 0\n",
    "        for j in range(len(estimated_arr)):\n",
    "            if estimated_arr[j] <= i:\n",
    "                num += 1\n",
    "        result.append(num/len(estimated_arr))    \n",
    "    return result\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "true_result = true_proportion_at_most(true_arr,2000)\n",
    "estimated_result = estimated_proportion_at_most(estimated_arr,2000)\n",
    "\n",
    "plt.clf()\n",
    "plt.semilogy(range(2001),estimated_result,\"-b\",label = \"Estimated\")\n",
    "plt.semilogy(range(2001),true_result,\"-r\",label=\"true\")\n",
    "plt.legend(loc=\"upper right\")\n",
    "plt.title('Estimated vs True result')\n",
    "plt.savefig(\"qsn5.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 6\n",
    "We will now move away from Markov chains and look towards another use of linear algebra and graphs: clustering algorithms. More specifically, in this exercise we will implement a simple spectral clustering algorithm: we will demonstrate this this algorithm works on a classic synthetic dataset: the 'nested moons' dataset. We will first generate this dataset and identify the ground truth clustering we seek to find. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.legend.Legend at 0x7fd397d2bf10>"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import sklearn.datasets\n",
    "\n",
    "n = 1000\n",
    "X,y = sklearn.datasets.make_moons(n, noise=0.05, random_state = 0)\n",
    "\n",
    "plt.scatter(X[y==0, 0], X[y==0, 1],label=\"True Cluster 1\")\n",
    "plt.scatter(X[y==1, 0], X[y==1, 1],label=\"True Cluster 2\")\n",
    "\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This dataset consists of $n=1000$ data points in 2-dimensional space. While it seems obvious to our eyes that a good partition of the data exists, many off-the-shelf clustering algorithms fail to fully identify the clusters on this dataset. As an example, here is how the standard K-means clustering algorithm fares on this dataset (we will see more examples of clustering in Lecture 7 when we cover Scikit-learn)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.legend.Legend at 0x7fd3b2654e90>"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "\n",
    "model = KMeans(n_clusters=2, random_state=0)\n",
    "kmeans_labels = model.fit(X).labels_\n",
    "\n",
    "plt.scatter(X[kmeans_labels==0, 0], X[kmeans_labels==0, 1],label=\"K-Means Cluster 1\")\n",
    "plt.scatter(X[kmeans_labels==1, 0], X[kmeans_labels==1, 1],label=\"K-Means Cluster 2\")\n",
    "    \n",
    "    \n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### part (a)\n",
    "$\\newcommand{\\norm}[1]{\\left\\lVert#1\\right\\rVert}$\n",
    "In this exercise, we will implement a basic spectral clustering algorithm which will correctly identify the clusters on this dataset. To begin, we will need to form an \"affinity matrix\": the $(i,j)$ entry of this matrix indicates how close together the $i^{th}$ and $j^{th}$ data points are in space. \n",
    "\n",
    "Let $x_i$ denote the $i^{th}$ point in the data set. Form an $n \\times n$ matrix $A$ where $A_{ij} = e^{-10 \\norm{x_i - x_j}}$ (this constant of $10$ is chosen because it gives good results for this task: it may be tuned in practice). (Here, $\\norm{x_i - x_j}$ is the vector $2$-norm of the difference between $x_i$ and $x_j$.) If you did this correctly, the minimum value in $A$ should be `1.679177168057234e-14`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.679177168057237e-14\n"
     ]
    }
   ],
   "source": [
    "#YOUR CODE HERE\n",
    "from numpy.linalg import norm\n",
    "import math as m\n",
    "\n",
    "def difference(xi,xj):\n",
    "    l2 = norm(np.subtract(xi,xj))\n",
    "    l2 *= -10\n",
    "    return m.e**l2\n",
    "\n",
    "def create_affinity_matrix(X):\n",
    "    A = np.empty((len(X),len(X)))\n",
    "\n",
    "    for i in range(len(X)):\n",
    "        for j in range(len(X)):\n",
    "            A[i][j] = difference(X[i],X[j])\n",
    "    return A\n",
    "\n",
    "A = create_affinity_matrix(X)\n",
    "print(A.min())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With this, we generate a [Laplacian matrix](https://www.cs.yale.edu/homes/spielman/561/2012/lect02-12.pdf). We do this in two steps: we first construct a diagonal matrix $D$ which contains the row sums of $A$ on its diagonal, and then we create the Laplacian $L = D- A$.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "D =  np.diag(A.sum(axis=0))\n",
    "L = D-A"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The algorithmic task behind spectral clustering algorithms is to find the eigenvector associated with *second smallest* eigenvalue of the matrix $L$. (All eigenvalues of $L$ are positive, and it is a simple exercise to show that the all-ones vector is an eigenvector of $L$ with eigenvalue $0.$) As we will later see, the entries of this vector will enable us to identify the partition in our original dataset. We will build up to this functionality in stages. \n",
    "\n",
    "We begin by writing a linear system solver for the matrix $L$. We will use an adaptation of [Richardson iteration](https://en.wikipedia.org/wiki/Modified_Richardson_iteration) to do this task: the pseudocode for this is given below. \n",
    "\n",
    "```\n",
    "    input) L: an n x n Laplacian matrix, b: an n x 1 vector with entries summing to 0, T: number of iterations\n",
    "    outputs) x: n x 1 vector approximately satisfying Lx =b.\n",
    "\n",
    "    x <- all-zeros vector\n",
    "    steplength = 0.03\n",
    "    for i = 0,1,2,...,T-1:\n",
    "        residual <- b - L*x\n",
    "        x <- x + steplength*residual\n",
    "```\n",
    "\n",
    "Write a function `richardson(L,b,T=500)` which implements the algorithm given above. (Note that the `steplength` parameter of $0.03$ may be optimized slightly, but this value gives good performance for solving linear systems in the Laplacian defined earlier.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "#YOUR CODE HERE\n",
    "def richardson(L,b,T=500): #in our later code, we will run for T = 500 iterations\n",
    "    x = np.zeros(len(b))\n",
    "    steplength = 0.03\n",
    "    for i in range(T):\n",
    "        residual = b - np.dot(L,x)\n",
    "        x = x + steplength * residual\n",
    "    return x\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### part (b)\n",
    "\n",
    "While the Richardson iteration above works well on small problems, it converges slowly on larger instances. This is especially true for our Laplacian linear system: the norm of our residual error may remain higher than $10^{-4}$ even after 1000 iterations. To address this issue, we will implement a version of [Nesterov acceleration](https://calculus.subwiki.org/wiki/Nesterov%27s_gradient_acceleration) to improve the convergence of this procedure. The pseudocode for this algorithm is given below: \n",
    "\n",
    "\n",
    "```\n",
    "    input) L: an n x n Laplacian matrix, b: an n x 1 vector with entries summing to 0, tol: error tolerance\n",
    "    outputs) x_T: n x 1 vector approximately satisfying Lx =b.\n",
    "\n",
    "    x_0, y_0 <- all-zeros vector\n",
    "    steplength = 0.03\n",
    "    alpha = 0.9\n",
    "    for i = 0, 1, ... T-1:\n",
    "        x_{i+1} <- y_i + steplength*(b - L*y_i)\n",
    "        y_{i+1} = x_{i+1} + alpha*(x_{i+1} - x_i)\n",
    "        i = i + 1\n",
    "```\n",
    "\n",
    "\n",
    "Here the index $i$ denotes the index of iteration: $y_1$ is a different $n$-dimensional vector to $y_0$ (and likewise with $x$). It does not indicate It may be helpful to un-roll the for loop to see the vectors it defines: if $\\eta$ is the steplength, we see\n",
    "\n",
    "\\begin{align*}\n",
    "x_1 = y_0 + \\eta(b- L y_0) \\\\\n",
    "y_1 = x_1 + \\alpha (x_1 - x_0) \\\\\n",
    "x_2 = y_1 + \\eta (b- L y_1) \\\\\n",
    "y_2 = x_2 + \\alpha (x_2 - x_1) \\\\\n",
    "x_3 = y_2 + \\eta (b- L y_2) \\\\\n",
    "y_3 = x_3 + \\alpha (x_3 - x_2)\n",
    "\\end{align*}\n",
    "\n",
    "and so on. We see that the vector $x_1$ defined on the first line is used to define both $y_1$ on the second line and $y_2$ on the fourth line. You should think of the algorithm as constructing two sequences of vectors $x_i$ and $y_i$: each $x$ vector is a function of the previous $y$ vector, but each $y$ vector is a function of the previous **two** $x$ vectors. You may find it helpful to store two different $x$ vectors (called $x_{current}$ and $x_{old}$ for example) to implement the algorithm.\n",
    "\n",
    "(For those interested in the justification of this procedure, solving the linear equations $Lx = b$ is equivalent to finding the minimizing $x$ for the quadratic $\\frac{1}{2} x^\\top L x - b^\\top x$. In the pseudocode, we apply the Nesterov accelerated gradient method to this problem. The constants of `alpha` and `steplength` can again be slightly improved, but these give good performance for our Laplacian)\n",
    "\n",
    "Write a function `agd(L,b,T=500)` to implement the above pseudocode. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "#YOUR CODE HERE\n",
    "def agd(L,b,T=500):\n",
    "    X = np.empty(L.shape)\n",
    "    Y = np.empty(L.shape)\n",
    "    X[0,:] = np.zeros(len(b))\n",
    "    Y[0,:] = np.zeros(len(b))\n",
    "    steplength = 0.03\n",
    "    alpha = 0.9\n",
    "\n",
    "    for i in range(T):\n",
    "        X[i+1,:] = Y[i] + steplength*(b - np.dot(L,Y[i]))\n",
    "        Y[i+1,:] = X[i+1] + alpha*(X[i+1] - X[i])\n",
    "\n",
    "    return X[T]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-0.03187048  0.034967   -0.01801166 -0.05511856 -0.01084392  0.06265154\n",
      " -0.04338873 -0.0137001  -0.05142273  0.05757941 -0.06668733  0.0152211\n",
      " -0.02367979 -0.0502513  -0.02664296 -0.02985648  0.06252664 -0.03977065\n",
      " -0.06041282 -0.04938189]\n",
      "\n",
      "[-0.03412209  0.03799348 -0.01441059 -0.0565506  -0.00688549  0.06389532\n",
      " -0.04473812 -0.01663341 -0.05383585  0.06030909 -0.06898215  0.01559058\n",
      " -0.0202497  -0.05244647 -0.02911922 -0.03319584  0.06468133 -0.04148352\n",
      " -0.06150148 -0.05137993]\n"
     ]
    }
   ],
   "source": [
    "#TESTING-- try using Richardson and AGD to solve the linear system Lx = v\n",
    "np.random.seed(0)\n",
    "v = np.random.rand(n)\n",
    "v -= np.sum(v)/n\n",
    "\n",
    "x_richard = richardson(L,v)\n",
    "x_agd = agd(L,v)\n",
    "\n",
    "print(x_richard[-20:], end=\"\\n\\n\")\n",
    "print(x_agd[-20:])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you did this correctly, you will see that the Nesterov acceleration method converges dramatically faster than Richardson iteration. \n",
    "\n",
    "We will now try to quantify this effect. Modify your implementations of `richardson` and `agd` to additionally return an array containing $\\norm{L x_i - b}$ for each $x_i$ encountered during the while loop of the algorithm. Compare these on a semilogy plot, and label your curves appropriately. Save the figure as `qsn6b.png`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "#YOUR CODE HERE\n",
    "def difference(xi,xj):\n",
    "    return norm(np.subtract(xi,xj))\n",
    "\n",
    "def modified_richardson(L,b,T=500):\n",
    "    result = []\n",
    "    x = np.zeros(len(b))\n",
    "    steplength = 0.03\n",
    "    for i in range(T):\n",
    "        residual = b - np.dot(L,x)\n",
    "        x = x + steplength * residual\n",
    "        result.append(difference(np.dot(L,x), b))\n",
    "\n",
    "    return result\n",
    "\n",
    "def modified_agd(L,b,T=500):\n",
    "    X = np.empty(L.shape)\n",
    "    Y = np.empty(L.shape)\n",
    "    X[0,:] = np.zeros(len(b))\n",
    "    Y[0,:] = np.zeros(len(b))\n",
    "    steplength = 0.03\n",
    "    alpha = 0.9\n",
    "\n",
    "    result = []\n",
    "\n",
    "    for i in range(T):\n",
    "        X[i+1,:] = Y[i] + steplength*(b - np.dot(L,Y[i]))\n",
    "        result.append(difference(np.dot(L,X[i+1]),b))\n",
    "        Y[i+1,:] = X[i+1] + alpha*(X[i+1] - X[i])\n",
    "    return result\n",
    "\n",
    "\n",
    "\n",
    "modi_rich_result = modified_richardson(L,v)\n",
    "modi_agd_result = modified_agd(L,v)\n",
    "\n",
    "plt.clf()\n",
    "plt.semilogy(range(500),modi_rich_result,\"-r\",label = \"Modified Richardson\")\n",
    "plt.semilogy(range(500),modi_agd_result,\"-b\",label=\"Modified AGD\")\n",
    "\n",
    "plt.legend(loc=\"upper right\")\n",
    "plt.title('Difference of Richardson and AGD')\n",
    "plt.savefig(\"qsn6b.png\")\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### part (c)\n",
    "\n",
    "\n",
    "We are finally ready to apply our linear system solver to spectral clustering. We do this via a modification of the power method from Lecture 4. We provide the pseudocode for this method below:\n",
    "\n",
    "```\n",
    "    input) L: an n x n Laplacian matrix, iters: number of iterations\n",
    "    outputs) l: the second-smallest eigenvalue of L\n",
    "             v: the corresponding eigenvector\n",
    "\n",
    "    v <- random vector of length n\n",
    "    v <-  v - sum(v)/n \n",
    "    #this ensures v is orthogonal to the all-ones vector (the smallest eigenvalue of L) \n",
    "    for i = 0,1,2...iters-1: \n",
    "        v <- x solving the equation L*x = v\n",
    "        v <-  v - sum(v)/n \n",
    "        #this ensures v is orthogonal to the all-ones vector (the smallest eigenvalue of L) \n",
    "        v <- v / ||v||_2\n",
    "        l <- v^T L v\n",
    "```\n",
    "\n",
    "Here, `||v||_2` denotes the vector $2$-norm (there is a function in numpy to compute this). Write a function `pow_method(L,iters=20)` which implements the above pseudocode and solves the linear systems using your `agd` implementation from part (b). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pow_method(L,iters=20):\n",
    "    pass\n",
    "\n",
    "l, v = pow_method(L)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the matrix $L$ in the problem, the eigenvalue you obtain should be between $0.1$ and $0.2$. \n",
    "\n",
    "We are finally ready to implement spectral clustering. To do this, use the function `np.argsort` to identify the indices of the largest $500$ entries from $v$. These will (hopefully) correspond to the elements of one of the clusters in our dataset. Additionally, compute the indices of the smallest $500$ entries of $v$: these will give the datapoints belonging to the other cluster. \n",
    "\n",
    "Modify the plotting code from the K-means example from earlier to display the computed spectral clusters of our input data. Label your plot appropriately, and save the figure as `qsn6c.png`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Submission Instructions\n",
    "\n",
    "You need to submit the following in canvas\n",
    "1. This notebook with the code filled in for each question\n",
    "2. The figures\n",
    "    1. qsn3.png\n",
    "    2. qsn4c.png and qsn4c_semilogy.png\n",
    "    3. qsn5.png\n",
    "    4. qsn6b.png\n",
    "    5. qsn6c.png"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
